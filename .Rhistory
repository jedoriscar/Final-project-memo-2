library(stm)
library(tm)
library(dplyr)
library(readr)
library(tibble)
library(readxl)
library(ggplot2)
library(wordcloud)
library(tidytext)
options(scipen = 100)
yt_dt <- read.csv("~/Documents/stat 302-1/Final-project-memo-2/youtube_data.csv")
yt_dt %>%
mutate(video_published_at = ymd_hms(video_published_at)) %>%
group_by(video_published_date = as.Date(video_published_at), video_title) %>%
summarise(total_likes = sum(likes)) %>%
arrange(desc(total_likes)) %>%
slice(1)
library(lubridate)
yt_dt %>%
mutate(video_published_at = ymd_hms(video_published_at)) %>%
group_by(video_published_date = as.Date(video_published_at), video_title) %>%
summarise(total_likes = sum(likes)) %>%
arrange(desc(total_likes)) %>%
slice(1)
yt_dt %>%
mutate(video_published_at = as.POSIXct(video_published_at, format = "%Y-%m-%d %H:%M:%S")) %>%
arrange(video_published_at, video_title)
library(lubridate)
yt_dt <- yt_dt %>%
mutate(comment_published_at = ymd_hms(comment_published_at),
comment_day_of_week = weekdays(comment_published_at),
comment_hour_of_day = hour(comment_published_at))
yt_dt %>%
count(comment_day_of_week) %>%
arrange(desc(n))
# Explore the distribution of comments over hours of the day
yt_dt %>%
count(comment_hour_of_day) %>%
arrange(desc(n))
yt_dt <- yt_dt %>%
mutate(video_published_at = ymd_hms(video_published_at),
video_day_of_week = weekdays(video_published_at),
video_hour_of_day = hour(video_published_at))
yt_dt <- yt_dt %>%
mutate(video_published_at = ymd_hms(video_published_at),
video_day_of_week = weekdays(video_published_at),
video_hour_of_day = hour(video_published_at))
yt_dt %>%
group_by(video_title, video_day_of_week) %>%
count() %>%
arrange(video_title, desc(n))
yt_dt %>%
group_by(video_title, video_hour_of_day) %>%
count() %>%
arrange(video_title, desc(n))
yt_dt %>%
mutate(video_published_at = ymd_hms(video_published_at)) %>%
group_by(video_published_date = as.Date(video_published_at), video_title) %>%
summarise(total_likes = sum(like_count)) %>%
arrange(desc(total_likes)) %>%
slice(1)
# Preprocess the comment column in yt_dt
yt_dt$comment_clean <- tolower(yt_dt$comment)  # Convert to lowercase
yt_dt$comment_clean <- gsub("\\d+", "", yt_dt$comment_clean)  # Remove numbers
yt_dt$comment_clean <- gsub("[[:punct:]]", "", yt_dt$comment_clean)  # Remove punctuation
yt_dt$comment_clean <- removeWords(yt_dt$comment_clean, stopwords("english"))  # Remove stopwords
yt_dt$comment_clean <- stripWhitespace(yt_dt$comment_clean)  # Remove extra whitespaces
# Stemming the words in comments
for (i in 1:nrow(yt_dt)) {
comment_words <- strsplit(yt_dt$comment_clean[i], " ")[[1]]  # Split the comment into individual words
stemmed_words <- stemDocument(comment_words)  # Find the stems of the words
yt_dt$comment_clean[i] <- paste(stemmed_words, collapse = " ")  # Replace the comment with the stemmed words
}
# Preprocess the comment column in yt_dt
yt_proc <- textProcessor(
documents = yt_dt$comment_clean,    # Specify the column containing the comments
metadata = yt_dt              # Include metadata if available (optional)
)
# Extract the components from yt_proc object
meta <- yt_proc$meta      # Extract the metadata from yt_proc
vocab <- yt_proc$vocab    # Extract the vocabulary from yt_proc
docs <- yt_proc$documents  # Extract the preprocessed documents from yt_proc
# Prepare the documents for further analysis
yt_out <- prepDocuments(docs, vocab, meta)
# Update the variables with the modified values
docs <- yt_out$documents  # Update the documents with the modified values from prepDocuments
vocab <- yt_out$vocab     # Update the vocabulary with the modified values from prepDocuments
meta <- yt_out$meta       # Update the metadata with the modified values from prepDocuments
# But how many times does each word appear?
yt_dt %>%
unnest_tokens(word, comment_clean) %>%
count(word, sort = TRUE) %>%
slice(1:25) # this is letting us the top 25 words
# Most frequent words by video:
yt_dt %>%
unnest_tokens(word, comment) %>%
anti_join(stop_words) %>%
count(video_title, word, sort = TRUE) %>%
group_by(video_title)
# filter(n > 1000) %>%
# Create a tidy text corpus
corpus <- yt_dt %>%
select(video_title, comment_clean) %>%
unnest_tokens(word, comment_clean)
# Calculate the frequency of each word
word_freq <- corpus %>%
count(video_title, word, sort = TRUE)
# Get the top n most frequent words for each title
n <- 1
top_words <- word_freq %>%
group_by(video_title) %>%
slice_max(n = n, order_by = n)
# Print the top words for each title
print(top_words)
# Create a tidy text corpus with bigrams
corpus <- yt_dt %>%
select(video_title, comment_clean) %>%
unnest_tokens(bigram, comment_clean, token = "ngrams", n = 2)
# Calculate the frequency of each bigram
bigram_freq <- corpus %>%
count(video_title, bigram, sort = TRUE)
# Get the top n most frequent bigrams for each title
n <- 1
top_bigrams <- bigram_freq %>%
group_by(video_title) %>%
slice_max(n = n, order_by = n)
# Print the top bigrams for each title
print(top_bigrams)
cor_matrix <- cor(yt_dt[, c("subscribers", "likes", "replies", "like_count", "comment_count", "comment_hour_of_day")])
# Convert the correlation matrix to a data frame for ggplot
cor_df <- as.data.frame(as.table(cor_matrix))
names(cor_df) <- c("Variable1", "Variable2", "Correlation")
# Plot heatmap using ggplot
ggplot(cor_df, aes(Variable1, Variable2, fill = Correlation)) +
geom_tile(color = "white") +
scale_fill_gradient2(low = "blue", high = "red", mid = "white",
midpoint = 0, limit = c(-1, 1), space = "Lab",
name="Correlation") +
geom_text(aes(label = round(Correlation, 2)), vjust = 1) +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, hjust = 1),
plot.title = element_text(hjust = 0.5)) +
ggtitle("Correlation Heatmap with Values")
library(wordcloud)
yt_dt %>%
with(wordcloud(video_title, max.words = 100, scale = c(3, 0.5)))
setwd("~/Documents/stat 302-1/Final-project-memo-2")
# Most frequent words by video:
yt_dt %>%
unnest_tokens(word, comment) %>%
anti_join(stop_words) %>%
count(video_title, word, sort = TRUE) %>%
group_by(video_title)
gc()
gc()
